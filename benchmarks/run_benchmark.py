"""Benchmark runner for Unbitrium standardized experiments.

This script loads benchmark configurations and executes experiments
with full provenance tracking.

Author: Olaf Yunus Laitinen Imanov <oyli@dtu.dk>
License: EUPL-1.2
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path

import yaml

from unbitrium.bench import BenchmarkConfig, BenchmarkRunner
from unbitrium.core.utils import get_provenance_info, set_global_seed


def load_config(config_path: Path) -> dict:
    """Load YAML configuration file."""
    with open(config_path) as f:
        return yaml.safe_load(f)


def run_benchmark(config_path: Path, output_dir: Path | None = None) -> None:
    """Run benchmark from configuration file."""
    # Load configuration
    raw_config = load_config(config_path)

    # Set seed
    seed = raw_config.get("reproducibility", {}).get("seed", 42)
    set_global_seed(seed)

    # Create BenchmarkConfig
    config = BenchmarkConfig.from_dict(raw_config)

    # Determine output directory
    if output_dir is None:
        output_dir = Path(raw_config.get("output", {}).get("dir", "./results"))
    output_dir.mkdir(parents=True, exist_ok=True)

    # Generate provenance manifest
    provenance = get_provenance_info()
    provenance["config_file"] = str(config_path)
    provenance["config"] = raw_config
    provenance["timestamp"] = datetime.now().isoformat()

    manifest_path = output_dir / "manifest.json"
    with open(manifest_path, "w") as f:
        json.dump(provenance, f, indent=2)
    print(f"Provenance manifest: {manifest_path}")

    # Run benchmark
    print(f"Running benchmark: {config.name}")
    print(f"Dataset: {config.dataset}")
    print(f"Clients: {config.num_clients}")
    print(f"Rounds: {config.num_rounds}")
    print("-" * 60)

    runner = BenchmarkRunner(config)
    results = runner.run()

    # Save results
    results_path = output_dir / "results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"Results saved: {results_path}")

    # Generate report
    if raw_config.get("output", {}).get("save_report", True):
        report = generate_report(config, results, provenance)
        report_path = output_dir / "report.md"
        with open(report_path, "w") as f:
            f.write(report)
        print(f"Report saved: {report_path}")

    print("\nBenchmark complete.")


def generate_report(config: BenchmarkConfig, results: dict, provenance: dict) -> str:
    """Generate Markdown benchmark report."""
    lines = [
        f"# Benchmark Report: {config.name}",
        "",
        f"**Generated**: {provenance['timestamp']}",
        f"**Config File**: {provenance.get('config_file', 'N/A')}",
        f"**Git Commit**: {provenance.get('git_commit', 'N/A')}",
        "",
        "## Configuration",
        "",
        f"- **Dataset**: {config.dataset}",
        f"- **Clients**: {config.num_clients}",
        f"- **Rounds**: {config.num_rounds}",
        f"- **Local Epochs**: {config.local_epochs}",
        f"- **Batch Size**: {config.batch_size}",
        f"- **Learning Rate**: {config.learning_rate}",
        "",
        "## Results Summary",
        "",
        "| Aggregator | Final Accuracy | Final Loss |",
        "|------------|----------------|------------|",
    ]

    for agg_name, metrics in results.items():
        acc = metrics.get("accuracy", [0])[-1]
        loss = metrics.get("loss", [0])[-1]
        lines.append(f"| {agg_name} | {acc:.4f} | {loss:.4f} |")

    lines.extend(
        [
            "",
            "---",
            "*Generated by Unbitrium Benchmark Runner*",
        ]
    )

    return "\n".join(lines)


def main() -> None:
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Run Unbitrium standardized benchmarks",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "config",
        type=Path,
        help="Path to benchmark configuration YAML file",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        default=None,
        help="Output directory (overrides config)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed (overrides config)",
    )

    args = parser.parse_args()

    if not args.config.exists():
        print(f"Error: Config file not found: {args.config}", file=sys.stderr)
        sys.exit(1)

    run_benchmark(args.config, args.output)


if __name__ == "__main__":
    main()
