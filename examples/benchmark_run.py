"""Full benchmark run demonstration.

This script demonstrates the standardized benchmark harness with
experiment provenance and result artifacts.

Author: Olaf Yunus Laitinen Imanov <oyli@dtu.dk>
License: EUPL-1.2
"""

from __future__ import annotations

import json
import os
from datetime import datetime
from pathlib import Path

import torch
import torch.nn as nn

from unbitrium.bench import BenchmarkRunner, BenchmarkConfig
from unbitrium.aggregators import FedAvg, FedProx, FedSim
from unbitrium.partitioning import DirichletPartitioner
from unbitrium.core.utils import set_global_seed, get_provenance_info


def main() -> None:
    """Run standardized benchmark with provenance tracking."""
    # Set deterministic seed
    set_global_seed(42)

    # Output directory
    output_dir = Path("benchmark_results")
    output_dir.mkdir(exist_ok=True)

    # Benchmark configuration
    config = BenchmarkConfig(
        name="cifar10_heterogeneity_sweep",
        dataset="synthetic",  # Replace with CIFAR-10 for real benchmarks
        num_clients=50,
        num_rounds=20,
        local_epochs=5,
        batch_size=32,
        learning_rate=0.01,
        partitioning={
            "strategy": "dirichlet",
            "alpha": 0.5,
        },
        aggregators=["fedavg", "fedprox", "fedsim"],
        metrics=["accuracy", "loss", "emd", "gradient_variance"],
        seed=42,
    )

    # Generate provenance manifest
    provenance = get_provenance_info()
    provenance["config"] = config.to_dict()
    provenance["timestamp"] = datetime.now().isoformat()

    manifest_path = output_dir / "manifest.json"
    with open(manifest_path, "w") as f:
        json.dump(provenance, f, indent=2)
    print(f"Provenance manifest saved to {manifest_path}")

    # Run benchmark
    runner = BenchmarkRunner(config)
    results = runner.run()

    # Save results
    results_path = output_dir / "results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"Results saved to {results_path}")

    # Generate human-readable report
    report = generate_report(config, results, provenance)
    report_path = output_dir / "report.md"
    with open(report_path, "w") as f:
        f.write(report)
    print(f"Report saved to {report_path}")

    # Summary
    print("\n" + "=" * 60)
    print("Benchmark Complete")
    print("=" * 60)
    for agg_name, metrics in results.items():
        final_acc = metrics.get("accuracy", [0])[-1]
        print(f"  {agg_name}: Final Accuracy = {final_acc:.4f}")


def generate_report(
    config: BenchmarkConfig,
    results: dict,
    provenance: dict,
) -> str:
    """Generate Markdown report."""
    lines = [
        f"# Benchmark Report: {config.name}",
        "",
        f"**Generated**: {provenance['timestamp']}",
        f"**Git Commit**: {provenance.get('git_commit', 'N/A')}",
        "",
        "## Configuration",
        "",
        "| Parameter | Value |",
        "|-----------|-------|",
        f"| Dataset | {config.dataset} |",
        f"| Clients | {config.num_clients} |",
        f"| Rounds | {config.num_rounds} |",
        f"| Local Epochs | {config.local_epochs} |",
        f"| Batch Size | {config.batch_size} |",
        f"| Learning Rate | {config.learning_rate} |",
        f"| Partitioning | {config.partitioning['strategy']} (alpha={config.partitioning['alpha']}) |",
        "",
        "## Results",
        "",
        "| Aggregator | Final Accuracy | Final Loss |",
        "|------------|----------------|------------|",
    ]

    for agg_name, metrics in results.items():
        acc = metrics.get("accuracy", [0])[-1]
        loss = metrics.get("loss", [0])[-1]
        lines.append(f"| {agg_name} | {acc:.4f} | {loss:.4f} |")

    lines.extend([
        "",
        "## Heterogeneity Metrics",
        "",
    ])

    if "emd" in results.get(list(results.keys())[0], {}):
        lines.append("EMD and gradient variance tracked per round.")

    lines.extend([
        "",
        "---",
        "",
        "*Report generated by Unbitrium Benchmark Harness*",
    ])

    return "\n".join(lines)


if __name__ == "__main__":
    main()
